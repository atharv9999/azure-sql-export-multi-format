{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01cd2e6-80e6-4cec-b983-50293fc5dcfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.atharvk9999storage.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.atharvk9999storage.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.atharvk9999storage.dfs.core.windows.net\", \n",
    "               \"Client ID\")  # client ID\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.atharvk9999storage.dfs.core.windows.net\", \n",
    "               \"Client Secret\") # secret\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.atharvk9999storage.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/Tenant_Id/oauth2/token\")  # tenant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169e1e8b-2ca1-4ce8-9d66-01cd8a6c19f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adls_path = \"abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d940f6-5db5-49c6-b1f3-6d30da2cb333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:sqlserver://azuresqlserver9999.database.windows.net:1433;database=azuresqldb\"\n",
    "db_props = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784ec635-9dce-4b4d-9838-e211ca0b748c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales table exported as single CSV to: abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/csvfiles/Sales.csv\n"
     ]
    }
   ],
   "source": [
    "table_name=\"Sales\"\n",
    "\n",
    "csv_temp_path = adls_path + f\"csvfiles_temp/{table_name}/\"\n",
    "csv_final_path = adls_path + f\"csvfiles/{table_name}.csv\"\n",
    "\n",
    "df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=db_props)\n",
    "\n",
    "df.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(csv_temp_path)\n",
    "\n",
    "files = dbutils.fs.ls(csv_temp_path)\n",
    "csv_file = [f for f in files if f.name.endswith(\".csv\")][0]\n",
    "\n",
    "dbutils.fs.mv(csv_file.path, csv_final_path)\n",
    "dbutils.fs.rm(csv_temp_path, recurse=True)\n",
    "\n",
    "print(f\"{table_name} table exported as single CSV to: {csv_final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd3bd345-1825-4c30-8a26-a940bfee88d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b544bf9-fc54-4429-8edc-32c473b808ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.option(\"header\", True).csv(csv_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa5a8a7-0fa6-4076-bd46-7200df1b0c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write Parquet as single file\n",
    "df_csv.coalesce(1).write.mode(\"overwrite\").parquet(parquet_temp_path)\n",
    "\n",
    "files = dbutils.fs.ls(parquet_temp_path)\n",
    "parquet_file = [f for f in files if f.name.endswith(\".parquet\")][0]\n",
    "\n",
    "dbutils.fs.mv(parquet_file.path, parquet_final_path)\n",
    "dbutils.fs.rm(parquet_temp_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bf5ddd-e792-4333-a28a-65a5047f0479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write Avro as single file\n",
    "df_csv.coalesce(1).write.mode(\"overwrite\").format(\"avro\").save(avro_temp_path)\n",
    "\n",
    "files = dbutils.fs.ls(avro_temp_path)\n",
    "avro_file = [f for f in files if f.name.endswith(\".avro\")][0]\n",
    "\n",
    "dbutils.fs.mv(avro_file.path, avro_final_path)\n",
    "dbutils.fs.rm(avro_temp_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fb4751-026a-44f3-82b9-267e3ee8d26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet and Avro stored as single files:\n",
      "Parquet: abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/parquetfiles/Sales.parquet\n",
      "Avro   : abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/avrofiles/Sales.avro\n"
     ]
    }
   ],
   "source": [
    "print(\"Parquet and Avro stored as single files:\")\n",
    "print(f\"Parquet: {parquet_final_path}\")\n",
    "print(f\"Avro   : {avro_final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce00fcab-6d9c-4cb6-8a1d-965946f904d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/csvfiles_temp\n",
      "Deleted: abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/parquetfiles_temp\n",
      "Deleted: abfss://rawfiles@atharvk9999storage.dfs.core.windows.net/avrofiles_temp\n"
     ]
    }
   ],
   "source": [
    "parquet_temp_path = adls_path + \"parquetfiles_temp\"\n",
    "avro_temp_path = adls_path + \"avrofiles_temp\"\n",
    "csv_temp_path = adls_path + \"csvfiles_temp\"\n",
    "\n",
    "for temp_path in [csv_temp_path, parquet_temp_path, avro_temp_path]:\n",
    "    try:\n",
    "        dbutils.fs.rm(temp_path, recurse=True)\n",
    "        print(f\"Deleted: {temp_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete {temp_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1350011-b11e-4753-b297-5ca387166edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL - Azure SQL DB -> ADLC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
